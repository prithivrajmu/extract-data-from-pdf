#!/usr/bin/env python3
"""
Diagnostic script to check Chandra OCR model loading status and provide recommendations.
"""

import os
import subprocess
import sys
from pathlib import Path


def check_model_files():
    """Check if model files are complete."""
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub" / "models--datalab-to--chandra"
    
    print("=" * 70)
    print("Chandra OCR Model Status Check")
    print("=" * 70)
    print()
    
    if not cache_dir.exists():
        print("‚ùå Model cache directory not found")
        print("   The model has not been downloaded yet.")
        return False
    
    # Find snapshot directory
    snapshots_dir = cache_dir / "snapshots"
    if not snapshots_dir.exists():
        print("‚ùå No snapshots found in cache")
        return False
    
    snapshots = list(snapshots_dir.iterdir())
    if not snapshots:
        print("‚ùå No snapshots found")
        return False
    
    snapshot = snapshots[0]
    print(f"üìÅ Snapshot: {snapshot.name}")
    print()
    
    # Check for model shards
    model_files = sorted(list(snapshot.glob("model-*-of-*.safetensors")))
    index_file = snapshot / "model.safetensors.index.json"
    
    if not index_file.exists():
        print("‚ùå Model index file not found")
        return False
    
    # Read index to see how many shards expected
    import json
    try:
        with open(index_file) as f:
            index_data = json.load(f)
            # Find unique shard filenames from weight_map
            weight_map = index_data.get("weight_map", {})
            shard_files = set()
            for weight_name, shard_file in weight_map.items():
                shard_files.add(shard_file)
            total_shards = len(shard_files) if shard_files else 4
    except Exception as e:
        # Fallback: check existing files
        total_shards = 4  # Default for this model
    
    print(f"üìä Expected model shards: {total_shards}")
    print(f"üì¶ Found model shards: {len(model_files)}")
    print()
    
    # Check each shard
    missing = []
    incomplete = []
    complete = []
    
    for i in range(1, total_shards + 1):
        shard_file = snapshot / f"model-{i:05d}-of-{total_shards:05d}.safetensors"
        if shard_file.exists():
            size = shard_file.stat().st_size
            if size < 1000:  # Symlink to blob
                # Check actual blob
                if shard_file.is_symlink():
                    blob_path = shard_file.readlink()
                    if not blob_path.is_absolute():
                        blob_path = cache_dir / "blobs" / blob_path.name
                    if blob_path.exists():
                        blob_size = blob_path.stat().st_size
                        if blob_size > 1000000:  # > 1MB
                            complete.append(i)
                        else:
                            incomplete.append(i)
                    else:
                        incomplete.append(i)
                else:
                    incomplete.append(i)
            else:
                complete.append(i)
        else:
            missing.append(i)
    
    print("Model shard status:")
    if complete:
        print(f"  ‚úÖ Complete: {len(complete)} shards {complete}")
    if incomplete:
        print(f"  ‚è≥ Incomplete: {len(incomplete)} shards {incomplete}")
    if missing:
        print(f"  ‚ùå Missing: {len(missing)} shards {missing}")
    print()
    
    # Check for incomplete downloads
    blobs_dir = cache_dir / "blobs"
    if blobs_dir.exists():
        incomplete_files = list(blobs_dir.glob("*.incomplete"))
        if incomplete_files:
            print(f"‚ö†Ô∏è  Active downloads detected: {len(incomplete_files)}")
            for inc_file in incomplete_files:
                size = inc_file.stat().st_size / (1024**3)  # GB
                print(f"   ‚Ä¢ {inc_file.name[:20]}... ({size:.2f} GB)")
    
    # Check running processes
    print()
    print("Running processes:")
    result = subprocess.run(
        ["pgrep", "-f", "chandra.*hf"],
        capture_output=True,
        text=True
    )
    if result.returncode == 0:
        pids = result.stdout.strip().split()
        print(f"  ‚ö†Ô∏è  Found {len(pids)} running chandra processes: {', '.join(pids)}")
        print("  üí° Tip: Having multiple processes can slow down download/loading")
    else:
        print("  ‚úì No running chandra processes")
    
    # Check GPU
    print()
    print("GPU Status:")
    result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
    if result.returncode == 0:
        print("  ‚úÖ GPU detected and available")
        print("  üí° GPU will significantly speed up model loading and inference")
    else:
        print("  ‚ö†Ô∏è  No GPU detected - using CPU only")
        print("  ‚ö†Ô∏è  CPU inference will be MUCH slower (10-20x slower)")
        print("  üí° Consider using GPU for faster processing")
    
    print()
    print("=" * 70)
    
    if missing or incomplete:
        print("üì• Model is still downloading or incomplete")
        print("   Estimated time remaining: 5-15 minutes depending on connection")
        print("   Recommended: Wait for download to complete before processing")
        return False
    else:
        print("‚úÖ Model files are complete!")
        return True


def check_memory():
    """Check available system memory."""
    print()
    print("System Memory:")
    try:
        result = subprocess.run(["free", "-h"], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        if len(lines) >= 2:
            mem_line = lines[1].split()
            total = mem_line[1]
            used = mem_line[2]
            available = mem_line[6] if len(mem_line) > 6 else mem_line[3]
            print(f"  Total: {total}")
            print(f"  Used: {used}")
            print(f"  Available: {available}")
            print()
            print("  üí° Model requires ~18GB RAM for CPU inference")
            print("  üí° With GPU: requires ~10GB GPU VRAM")
    except (OSError, subprocess.SubprocessError) as error:
        print("  ‚ö†Ô∏è  Could not check memory")
        print(f"      Details: {error}")


def recommendations():
    """Provide optimization recommendations."""
    print()
    print("=" * 70)
    print("Recommendations:")
    print("=" * 70)
    print()
    print("1. üõë If model is still downloading:")
    print("   ‚Ä¢ Wait for download to complete (check progress above)")
    print("   ‚Ä¢ Don't run multiple chandra processes simultaneously")
    print()
    print("2. ‚ö° To speed up model loading:")
    print("   ‚Ä¢ Use GPU if available (10-20x faster)")
    print("   ‚Ä¢ Ensure sufficient RAM (18GB+ for CPU, 10GB+ VRAM for GPU)")
    print("   ‚Ä¢ Close other memory-intensive applications")
    print()
    print("3. üöÄ After model is loaded:")
    print("   ‚Ä¢ First inference is slower (JIT compilation)")
    print("   ‚Ä¢ Subsequent runs will be faster")
    print("   ‚Ä¢ Consider using batch processing for multiple PDFs")
    print()
    print("4. üíæ Cache location:")
    print(f"   {Path.home() / '.cache' / 'huggingface' / 'hub' / 'models--datalab-to--chandra'}")
    print("   ‚Ä¢ Model files are ~18GB total")
    print("   ‚Ä¢ Once downloaded, cached locally for future use")
    print()


if __name__ == "__main__":
    complete = check_model_files()
    check_memory()
    recommendations()
    
    if not complete:
        print()
        print("‚ö†Ô∏è  Model download/loading is in progress.")
        print("   Please wait for it to complete before processing PDFs.")
        sys.exit(1)
    else:
        print()
        print("‚úÖ Model is ready! You can proceed with extraction.")
        sys.exit(0)

